ID3决策树分类

给定数据集，请用ID3决策树分类方法计算数据集的决策规则，数据全部是离散化数据。

信息增益相同的，按属性出现早的优先。


输入格式：

输入多行直至空行或结束

第一行，多个字段，每个字段都是字符串，字符串用空格隔开，第一个字段表示样本编号，最后一个字段是目标属性，其它字段是样本属性（样本属性总数<10）

接着每行是一个样本数据，第一个数据是编号，其它是属性值，没有空缺，而且都是字符串，它们之间用空格隔开。

样本数<500


输出格式：

一行，一个决策规则，决策规则从根节点到叶子结点，格式是“属性名=属性值#+#属性名=属性值#===>#目标属性名=目标属性值“，其中“#”是指空格 。

决策规则输出顺序按字典序输出。


输入样例：

ID House Married Salary Default

1 yes single >80 no

2 no married >80 no

3 no single <80 no

4 yes married >80 no

5 no divorced >80 yes

6 no married <80 no

7 yes divorced >80 no

8 no single >80 yes

9 no married <80 no

10 no single >80 yes


输出样例：

Married=divorced + House=no ===> Default=yes

Married=divorced + House=yes ===> Default=no

Married=married ===> Default=no

Married=single + House=no + Salary=<80 ===> Default=no

Married=single + House=no + Salary=>80 ===> Default=yes

Married=single + House=yes ===> Default=no


我的答案：

    import math
    from collections import Counter, defaultdict

    # 计算信息增益
    def gain(d, f, t):
        te = calc_entropy(d, t)
        n = len(d)
        s = defaultdict(list)

        for r in d:
            s[r[f]].append(r)

        we = sum((len(sub) / n) * calc_entropy(sub, t) for sub in s.values())
        return te - we

    # 计算熵
    def calc_entropy(d, t):
        n = len(d)
        c = Counter(r[t] for r in d)
        return -sum((v / n) * math.log2(v / n) for v in c.values() if v > 0)

    # 递归构建决策树
    def id3(d, a, t, p=""):
        tv = [r[t] for r in d]

        if len(set(tv)) == 1:
            return [f"{p} ===> {h[t]}={tv[0]}"]

        if not a:
            mc = Counter(tv).most_common(1)[0][0]
            return [f"{p} ===> {h[t]}={mc}"]

        ba = max(a, key=lambda i: gain(d, i, t))
        ban = h[ba]

        ps = defaultdict(list)
        for r in d:
            ps[r[ba]].append(r)

        rs = []
        ra = [i for i in a if i != ba]

        for v, sd in sorted(ps.items()):
            np = f"{p} + {ban}={v}" if p else f"{ban}={v}"
            rs.extend(id3(sd, ra, t, np))

        return rs

    # 主函数
    def main():
        d = []

        while True:
            try:
                l = input().strip()
                if not l:
                    break
                d.append(l.split())
            except EOFError:
                break

        global h
        h = d[0]
        d = d[1:]
        t = len(h) - 1
        a = list(range(1, t))

        rs = id3(d, a, t)
        for r in sorted(rs):
            print(r)

    if __name__ == "__main__":
        main()

正确答案：
程序语言 DEV C++
~~~~C++
#include <bits/stdc++.h>

using namespace std;


int N, M=10, K;


typedef vector<string> VS;

typedef vector<VS> VVS;


VVS data;//实例集

VS target_attr;//保存首行即属性行数据

string blank("");

map<string,VS> attr2values;//存储属性对应的所有的值


struct Node { //决策树节点

    string attribute;//属性值

    string arrived_value;//到达的属性值

    vector<Node *> childs;//所有的孩子

    Node(): attribute(blank), arrived_value(blank) {};

};

Node *root=NULL;


//根据数据实例计算属性与值组成的map

void MapValue() {

    unsigned int i,j;

    vector<string> values;

    for(i = 1; i < M; i++) { //按照列遍历

        for (j = 1; j < data.size(); j++)

            if( !count(values.begin(),values.end(),data[j][i]) )

                values.push_back(data[j][i]);

        attr2values[data[0][i]]=values;

        values.clear();

    }

    K=attr2values[data[0][M-1]].size();

}


//根据具体属性和值来计算熵

double ComputeEntropy(VVS data, string attribute, string value,bool parent) {

    vector<int> cnt (K,0);

    unsigned int i,j,k;

    double sum=0;

    VS &vatt=attr2values[target_attr[M-1]];

    for(j = 1; j < M; j++) {

        if(target_attr[j].compare(attribute)) continue;

        for(i = 1; i < data.size(); i++)

            if((!parent&&data[i][j]==value) || parent) { //parent记录是否算父节点

                k=find(vatt.begin(),vatt.end(),data[i][M-1])-vatt.begin();

                cnt[k] ++, sum++;

            };

        break;

    }

    double entropy=0.0;

    for(k=0; k<K; k++) {

        if(cnt[k] == 0 ) continue;

        entropy += -cnt[k]/sum*log(cnt[k]/sum)/log(2.0);

    }

    return entropy;

}


//计算按照属性attribute划分当前剩余实例的信息增益

double ComputeGain(VVS data, string attribute) {

    unsigned int j,k,m;

    //首先求不做划分时的熵

    double parent_entropy = ComputeEntropy(data, attribute, blank, true);

    double children_entropy = 0;

    //然后求做划分后各个值的熵

    VS values = attr2values[attribute];

    vector<int> cnt;

    int tempint;

    for(m = 0; m < values.size(); m++) {

        tempint = 0;

        for(k = 1; k < M - 1; k++)

            if(target_attr[k]==attribute)

                for(j = 1; j < data.size(); j++)

                    if(data[j][k]==values[m])

                        tempint++;

        cnt.push_back(tempint);

    }


    double subentropy;

    for(j = 0; j < values.size(); j++) {

        subentropy = ComputeEntropy(data, attribute, values[j], false);

        children_entropy += cnt[j] * subentropy / (data.size()-1);

    }

    return (parent_entropy - children_entropy);

}


int FindAttriIdx(string attri) {

    for(int i = 0; i < M; i++)

        if(data[0][i]==attri) return i;

    return 0;

}


//找出样例中占多数的正/负性

string MostLabel(VVS &data) {

    vector<int> cnt(K,0);

    int mostidx, mostcnt;

    VS &vatt=attr2values[target_attr[M-1]];

    for(unsigned i = 1; i < data.size(); i++) {

        int k=find(vatt.begin(),vatt.end(),data[i][M-1])-vatt.begin();

        cnt[k] ++;

    }

    for(int k = 0; k < K; k++)

        if(!k || mostcnt<cnt[k]) mostcnt=cnt[k], mostidx=k;

    return vatt[mostidx];

}


//判断样例是否正负性都为label

string AllLabelSame(VVS &remain_data) {

    vector<int> cnt(K, 0);

    int mostidx, mostcnt;

    VS &vatt=attr2values[target_attr[M-1]];

    for(unsigned i = 1; i < remain_data.size(); i++) {

        int k=find(vatt.begin(),vatt.end(),remain_data[i][M-1])-vatt.begin();

        cnt[k] ++;

    }

    for(int k = 0; k < K; k++)

        if(!k || mostcnt<cnt[k]) mostcnt=cnt[k], mostidx=k;

    if( mostcnt != remain_data.size()-1 ) return blank;

    return vatt[mostidx];

}


//返回根结点指针

Node * BulidTree(Node *p, VVS data, VS attr) {

    if (p == NULL)  p = new Node();

    p->attribute = AllLabelSame(data);  //先看搜索到树叶的情况

    if(attr.size() == 0) p->attribute = MostLabel(data);

    if(p->attribute != blank) return p;


    double max_gain = -1., temp_gain;

    VS::iterator max_it, it;

    for(it = attr.begin(); it < attr.end(); it++) {

        temp_gain = ComputeGain(data, (*it));

        if(temp_gain>max_gain+1e-8)  max_gain=temp_gain,  max_it = it;

    }


    //下面根据max_it指向的属性来划分当前样例，更新样例集和属性集

    VS new_attribute;

    VVS new_data;

    for(it = attr.begin(); it < attr.end(); it++)

        if(it->compare(*max_it)) new_attribute.push_back(*it);


    //确定了最佳划分属性，注意保存

    p->attribute = *max_it;

    VS values = attr2values[*max_it];

    int idx_attr = FindAttriIdx(*max_it);

    new_data.push_back(target_attr);

    for(it = values.begin(); it < values.end(); it++) {

        for(unsigned int i = 1; i < data.size(); i++)

            if(data[i][idx_attr]==*it)

                new_data.push_back(data[i]);


        Node * new_node = new Node();

        new_node->arrived_value = *it;

        if(new_data.size()==1)continue;

        BulidTree(new_node, new_data, new_attribute);

        p->childs.push_back(new_node);//将新结点加入父节点孩子容器

        new_data.clear(), new_data.push_back(target_attr);//重置new_data容器

    }

    return p;

}


set<string> decision;

void PrintTree(Node *p, string s) {

    bool leaf=p->childs.empty();

    if(!p->arrived_value.empty())

        s=s+p->arrived_value + (!leaf ? " + " : "");

    if(leaf) s=s+" ===> "+p->attribute,  decision.insert(s);

    for(int i=0; i<p->childs.size(); i++)

        PrintTree(p->childs[i], s);

}


int main() {

    //freopen("CH3\\ID3\\titanic2.txt","r",stdin);


    string a,b;

    while(getline(cin,a)) {

        stringstream ss(a);

        vector<string> vs;

        while(ss>>b) vs.push_back(b);

        if(vs.empty())break;

        if(N++)

            for(int m=1; m<data[0].size(); m++)

                vs[m]=data[0][m]+"="+vs[m];

        data.push_back(vs);

    }

    target_attr=data[0];

    M=data[0].size();


    VS remain_attr;

    for(int m=1; m<M-1; m++)

        remain_attr.push_back(data[0][m]);

    VVS remain_data;

    remain_data=data;


    MapValue();

    root = BulidTree(root,remain_data,remain_attr);


    a.clear();

    PrintTree(root,a);

    set<string>::iterator it;

    for(it=decision.begin(); it!=decision.end(); it++)

        cout<<*it<<endl;

    return 0;

}

~~~~

用例1：
输入

Day Outlook Temperature Humidity Wind PlayTennis 1 Sunny Hot High Weak no 2 Sunny Hot High Strong no 3 Overcast Hot High Weak yes 4 Rainy Mild High Weak yes 5 Rainy Cool Normal Weak yes 6 Rainy Cool Normal Strong no 7 Overcast Cool Normal Strong yes 8 Sunny Mild High Weak no 9 Sunny Cool Normal Weak yes 10 Rainy Mild Normal Weak yes 11 Sunny Mild Normal Strong yes 12 Overcast Mild High Strong yes 13 Overcast Hot Normal Weak yes 14 Rainy Mild High Strong no
输出

Outlook=Overcast ===> PlayTennis=yes Outlook=Rainy + Wind=Strong ===> PlayTennis=no Outlook=Rainy + Wind=Weak ===> PlayTennis=yes Outlook=Sunny + Humidity=High ===> PlayTennis=no Outlook=Sunny + Humidity=Normal ===> PlayTennis=yes
用例2：
输入

ID Age Income Married Education Buy 1 <30 High No Fair no 2 <30 High No Excellent no 3 30~40 High No Fair yes 4 >40 Medium No Fair yes 5 >40 Low Yes Fair yes 6 >40 Low Yes Excellent no 7 30~40 Low Yes Excellent yes 8 <30 Medium No Fair no 9 <30 Low Yes Fair yes 10 >40 Medium Yes Fair yes 11 <30 Medium Yes Excellent yes 12 30~40 Medium No Excellent yes 13 30~40 High Yes Fair yes 14 >40 Medium No Excellent no
输出

Age=30~40 ===> Buy=yes Age=<30 + Married=No ===> Buy=no Age=<30 + Married=Yes ===> Buy=yes Age=>40 + Education=Excellent ===> Buy=no Age=>40 + Education=Fair ===> Buy=yes
用例3：
输入

ID AGE PRESCR ASTIGMATISM TEAR-RATE LENSES 1 young myope no reduced no-lenses 2 young myope no normal soft 3 young myope yes reduced no-lenses 4 young myope yes normal hard 5 young hyper no reduced no-lenses 6 young hyper no normal soft 7 young hyper yes reduced no-lenses 8 young hyper yes normal hard 9 pre myope no reduced no-lenses 10 pre myope no normal soft 11 pre myope yes reduced no-lenses 12 pre myope yes normal hard 13 pre hyper no reduced no-lenses 14 pre hyper no normal soft 15 pre hyper yes reduced no-lenses 16 pre hyper yes normal no-lenses 17 presbyopic myope no reduced no-lenses 18 presbyopic myope no normal no-lenses 19 presbyopic myope yes reduced no-lenses 20 presbyopic myope yes normal hard 21 presbyopic hyper no reduced no-lenses 22 presbyopic hyper no normal soft 23 presbyopic hyper yes reduced no-lenses 24 presbyopic hyper yes normal no-lenses
输出

TEAR-RATE=normal + ASTIGMATISM=no + AGE=pre ===> LENSES=soft TEAR-RATE=normal + ASTIGMATISM=no + AGE=presbyopic + PRESCR=hyper ===> LENSES=soft TEAR-RATE=normal + ASTIGMATISM=no + AGE=presbyopic + PRESCR=myope ===> LENSES=no-lenses TEAR-RATE=normal + ASTIGMATISM=no + AGE=young ===> LENSES=soft TEAR-RATE=normal + ASTIGMATISM=yes + PRESCR=hyper + AGE=pre ===> LENSES=no-lenses TEAR-RATE=normal + ASTIGMATISM=yes + PRESCR=hyper + AGE=presbyopic ===> LENSES=no-lenses TEAR-RATE=normal + ASTIGMATISM=yes + PRESCR=hyper + AGE=young ===> LENSES=hard TEAR-RATE=normal + ASTIGMATISM=yes + PRESCR=myope ===> LENSES=hard TEAR-RATE=reduced ===> LENSES=no-lenses
用例4：
输入

ID Education Married Car Income Affordable 1 College single no medium no 2 College single no high no 3 College married no high yes 4 College married yes medium yes 5 College single no medium no 6 Under single no medium no 7 Under single no high no 8 Under married yes high yes 9 Under single yes very-high yes 10 Under single yes very-high yes 11 Post single yes very-high yes 12 Post single yes high yes 13 Post married no high yes 14 Post married no very-high yes 15 Post single no medium no
输出

Car=no + Married=married ===> Affordable=yes Car=no + Married=single ===> Affordable=no Car=yes ===> Affordable=yes
用例5：
输入

ID A B C D E F Sweet 1 2 2 2 1 3 1 yes 2 3 2 3 1 3 1 yes 3 3 2 2 1 3 1 yes 4 2 2 3 1 3 1 yes 5 1 2 2 1 3 1 yes 6 2 1 2 1 2 2 yes 7 3 1 2 2 2 2 yes 8 3 1 2 1 2 1 yes 9 3 1 3 2 2 1 no 10 2 3 1 1 1 2 no 11 1 3 1 3 1 1 no 12 1 2 2 3 1 2 no 13 2 1 2 2 3 1 no 14 1 1 3 2 3 1 no 15 3 1 2 1 2 2 no 16 1 2 2 3 1 1 no 17 2 2 3 2 2 1 no
输出

D=1 + B=1 + A=2 ===> Sweet=yes D=1 + B=1 + A=3 + F=1 ===> Sweet=yes D=1 + B=1 + A=3 + F=2 ===> Sweet=no D=1 + B=2 ===> Sweet=yes D=1 + B=3 ===> Sweet=no D=2 + F=1 ===> Sweet=no D=2 + F=2 ===> Sweet=yes D=3 ===> Sweet=no
