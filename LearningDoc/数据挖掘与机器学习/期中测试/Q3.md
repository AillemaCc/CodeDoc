逻辑回归

给定数据集，请用逻辑分类方法计算数据集的分类情况，数据是二维数据，分类是二类分类。h(x,y)=a+bx+cy+d*x*x+e*x*y+f*y*y，a/b/c/d/e/f就是要优化的变量θ，初始全部是0。h(x,y)就是sigmoid函数1/(1+exp(-h))中的h。学习率α=0.3，迭代次数3000次（不多也不少），用基于梯度的优化方法，导数用理论的导数公式。


输入格式：

第一行，三个整数，ns，np，nt。ns是样本数，np是属性个数（保证=2），nt是测试用例。

接着ns+nt行，前ns行是训练集数据，后nt行是测试集数据。每一行三个数据，前两个是实数，后一个是整数（1或0），表示数据分类。


输出格式：

一行，nt个测试集中的分类结果，分类概率大于0.55，输出“P ”（P后面有一个空格），分类概率在[0.45,0.55]（闭区间），输出“V ”（V后面有一个空格），分类概率小于0.45，输出“N ”（N后面有一个空格）。输出顺序，按测试集数据顺序。


输入样例：

20 2 3

-0.97 0.46 0 

0.86 -0.19 0 

0.6 0.83 1 

0.51 1.08 1 

-0.44 0.68 1 

0.37 -0.33 0 

0.23 -0.59 0 

-0.66 -0.48 0 

0.69 -0.04 0 

-0.9 0.99 1 

-0.97 1.15 1 

-0.49 -0.57 0 

-0.78 -0.35 0 

0.07 -0.9 0 

0.02 -0.22 0 

0.27 -0.9 0 

-0.02 -0.61 0 

0.65 0.2 0 

-0.54 -0.51 0 

0.95 0.55 0 

-0.9 1.38 1 

-0.08 0.37 1 

-0.42 0.04 0 


输出样例：

P V N

我的答案：

    import java.util.Scanner;
    import java.util.ArrayList;
    import java.util.Arrays;

    class LogisticRegression {

        // Sigmoid函数
        private static double sigmoid(double z) {
            return 1 / (1 + Math.exp(-z));
        }

        // 梯度下降算法
        private static double[] gradientDescent(double[][] X, int[] y, double[] theta, double alpha, int num_iters) {
            int m = y.length;

            for (int i = 0; i < num_iters; i++) {
                double[] z = new double[m];
                double[] h = new double[m];

                // 计算预测值
                for (int j = 0; j < m; j++) {
                    z[j] = 0;
                    for (int k = 0; k < X[j].length; k++) {
                        z[j] += X[j][k] * theta[k];
                    }
                    h[j] = sigmoid(z[j]);
                }

                // 计算误差
                double[] error = new double[m];
                for (int j = 0; j < m; j++) {
                    error[j] = h[j] - y[j];
                }

                // 计算梯度
                double[] gradient = new double[theta.length];
                for (int k = 0; k < theta.length; k++) {
                    double sum = 0;
                    for (int j = 0; j < m; j++) {
                        sum += error[j] * X[j][k];
                    }
                    gradient[k] = sum / m;
                }

                // 更新theta
                for (int k = 0; k < theta.length; k++) {
                    theta[k] -= alpha * gradient[k];
                }
            }

            return theta;
        }

        // 预测函数
        private static String[] predict(double[][] X, double[] theta) {
            int m = X.length;
            String[] predictions = new String[m];

            for (int i = 0; i < m; i++) {
                double z = 0;
                for (int j = 0; j < X[i].length; j++) {
                    z += X[i][j] * theta[j];
                }
                double h = sigmoid(z);

                if (h > 0.55) {
                    predictions[i] = "P ";
                } else if (h >= 0.45 && h <= 0.55) {
                    predictions[i] = "V ";
                } else {
                    predictions[i] = "N ";
                }
            }

            return predictions;
        }

        public static void main(String[] args) {
            Scanner scanner = new Scanner(System.in);

            // 读取样本数、属性个数、测试用例数
            int ns = scanner.nextInt();
            int np = scanner.nextInt();
            int nt = scanner.nextInt();

            // 存储所有数据
            ArrayList<double[]> dataList = new ArrayList<>();
            for (int i = 0; i < ns + nt; i++) {
                double[] data = new double[np + 1];
                for (int j = 0; j < np + 1; j++) {
                    data[j] = scanner.nextDouble();
                }
                dataList.add(data);
            }

            double[][] X_train = new double[ns][];
            int[] y_train = new int[ns];
            double[][] X_test = new double[nt][];

            // 划分训练集和测试集
            for (int i = 0; i < ns + nt; i++) {
                double[] data = dataList.get(i);
                if (i < ns) {
                    X_train[i] = new double[]{1, data[0], data[1], data[0] * data[0], data[0] * data[1], data[1] * data[1]};
                    y_train[i] = (int) data[2];
                } else {
                    X_test[i - ns] = new double[]{1, data[0], data[1], data[0] * data[0], data[0] * data[1], data[1] * data[1]};
                }
            }

            double[] theta = new double[6];
            double alpha = 0.3;
            int num_iters = 3000;

            // 执行梯度下降算法
            theta = gradientDescent(X_train, y_train, theta, alpha, num_iters);

            // 进行预测并输出结果
            String[] predictions = predict(X_test, theta);
            for (String prediction : predictions) {
                System.out.print(prediction);
            }

            scanner.close();
        }
    }

正确答案：
程序语言 DEV C++
~~~~c++
#include <bits/stdc++.h>

using namespace std;


int ns,np,nt;    // 样本数，特征数，测试数

double X[2000][10] ;

double y[2000],th0[10],th1[10],der[10];


double hypoVal(double x[]) { //计算假设函数的值

    double sum = 0;

    for (int i = 0; i < np; i++)

        sum += th0[i] * x[i];

    return 1.0 / (1.0 + exp(-sum));

}


int main() {

    //freopen(".\\LogReg\\dataset-logreg-4.txt","r",stdin);

    cin>>ns>>np>>nt;

    np=6;

    for (int i = 0; i < ns+nt; i++) {

        double a, b;

        cin >> a>>b;

        X[i][0]=1,   X[i][1]=a,   X[i][2]=b;

        X[i][3]=a*a, X[i][4]=a*b, X[i][5]=b*b;

        cin>>y[i];

    }


    double h, a=0.3, sum;

    // 梯度下降法求最优解

    int cnt = 0;

    do {

        for (int j = 0; j < np; j++) {

            sum=0;

            for (int i = 0; i < ns; i++)

                sum += (hypoVal(X[i]) - y[i]) * X[i][j];

            der[j] = (1.0 / double (ns)) * sum;

            th1[j] = th0[j] - a * der[j];

        }

        for (int j = 0; j < np; j++)

            th0[j] = th1[j];

        cnt++;

    } while (cnt<3000);


    for (int i = ns; i < ns+nt; i++) {

        h = hypoVal(X[i]);

        if(h>0.55)cout<<"P ";

        else if(h<0.45)cout<<"N ";

        else cout<<"V ";

    }

    cout<<endl;


    return 0;

}
~~~~


用例1：
输入

20 2 3 -0.97 0.46 0 0.86 -0.19 0 0.6 0.83 1 0.51 1.08 1 -0.44 0.68 1 0.37 -0.33 0 0.23 -0.59 0 -0.66 -0.48 0 0.69 -0.04 0 -0.9 0.99 1 -0.97 1.15 1 -0.49 -0.57 0 -0.78 -0.35 0 0.07 -0.9 0 0.02 -0.22 0 0.27 -0.9 0 -0.02 -0.61 0 0.65 0.2 0 -0.54 -0.51 0 0.95 0.55 0 -0.9 1.38 1 -0.08 0.37 1 -0.42 0.04 0
输出

P V N
用例2：
输入

50 2 10 -0.25 -0.73 0 -0.64 0.44 1 0.23 -0.64 0 0.62 0.39 1 -0.83 0.78 1 0.65 0.49 1 -0.36 0.6 1 -0.57 -0.08 0 0.12 0.56 1 0.97 0.48 0 -0.77 0.11 0 -0.74 -0.25 0 0.87 0.37 0 0.41 -0.51 0 0.31 -0.38 0 0.11 -0.31 0 -0.02 -0.85 0 0.55 0.3 0 0.22 -0.58 0 0.66 -0.09 0 0.68 -0.51 0 0.76 0.22 0 0.39 -0.37 0 0.82 0.45 0 0.58 -0.58 0 -0.32 -0.78 0 -0.77 1.36 1 -0.37 0.75 1 -0.43 0.82 1 -0.15 0.64 1 -0.38 0.73 1 0.15 0.6 1 0.95 1.87 1 0.45 -0.48 0 -0.25 -0.11 0 0.01 0.02 1 -0.89 0.18 0 0.82 1.27 1 -0.5 1.2 1 0.69 0.03 0 0.72 -0.43 0 0.21 0.69 1 0.79 0.19 0 0.28 0.57 1 -0.69 1.26 1 0.77 -0.11 0 -0.25 0.22 1 0.62 0.95 1 0.1 -0.81 0 0.29 0.44 1 0.1 -0.67 0 0.08 0.2 1 -0.37 -0.27 0 0.92 0.11 0 0.18 0.87 1 0.97 0.37 0 -0.6 -0.02 0 -0.38 0.4 1 -0.33 0.12 1 0.05 0.19 1
输出

N P N N P N N P P P
用例3：
输入

200 2 80 -0.78 0.55 0 0.72 0.41 0 0.81 0.36 0 -0.18 0.6 1 0.64 1.37 1 0.67 0.47 1 -0.91 0.03 0 -0.14 0.15 1 0.78 1.14 1 0.85 1.49 1 -0.02 0.03 1 0.12 0.67 1 -0.41 -0.82 0 -0.69 1.08 1 -0.77 -0.07 0 0.87 0.13 0 -0.42 0.99 1 0.2 -0.17 0 -0.35 0.53 1 -0.24 0.99 1 0.61 0.06 0 -0.32 -0.52 0 -0.4 0.28 1 0.96 1.76 1 -0.71 0.23 0 -0.14 -0.27 0 0.23 -0.94 0 0.74 0.34 0 -0.14 0.5 1 0.51 0.84 1 0.51 -0.19 0 -0.82 1.02 1 -0.21 0.45 1 -0.34 -0.06 0 -0.25 0.88 1 -0.01 0.95 1 0.99 0.78 0 0.92 1.72 1 0.02 -0.57 0 0.55 1.25 1 0.11 0.48 1 0.37 0.38 1 0.34 -0.14 0 0.94 0.14 0 -0.04 -0.67 0 -0.07 0.18 1 0.45 -0.61 0 0.01 0.45 1 -0.9 0.72 0 0.03 0.44 1 -0.52 0.4 1 -0.62 0.54 1 0.97 1.46 1 -0.66 0.57 1 -0.73 1.16 1 -0.01 -0.81 0 0.04 -0.23 0 -0.3 0.04 0 0.4 -0.52 0 -0.06 -0.48 0 -0.74 1.19 1 0.15 1.01 1 0.79 1.03 1 -0.42 -0.2 0 0.49 -0.7 0 -0.25 0.04 0 0.69 0.57 1 0.27 -0.73 0 0.35 0.79 1 -0.22 -0.03 0 -0.88 0.83 1 0.07 0.01 1 0.77 0.39 0 -0.8 0.93 1 -0.68 0.26 0 -0.28 0.35 1 0.96 1.39 1 -0.89 0.77 0 0.22 0.49 1 -0.46 0.25 1 0.68 0.32 0 -0.02 0.43 1 0.73 0.12 0 0.52 -0.03 0 0.64 1.35 1 -0.63 0.62 1 -0.13 0.27 1 0.48 -0.56 0 -0.5 0 0 -0.41 0.65 1 0.29 0 0 0.17 0.18 1 0.54 0.04 0 0.63 0.65 1 0.88 1.14 1 -0.63 0.61 1 -0.9 1.78 1 -0.87 0.72 0 0.24 0.94 1 0.44 -0.1 0 0.88 0.51 0 -0.12 0.29 1 -0.35 0.96 1 0.85 0.78 1 0.08 0.2 1 0.67 -0.11 0 0.45 0.34 1 0.08 0.16 1 0.06 -0.26 0 0.74 0.09 0 -0.89 0.44 0 0.47 1.01 1 -0.22 -0.51 0 -0.05 -0.82 0 0.42 -0.27 0 -0.05 -0.76 0 -0.44 -0.1 0 -0.44 0.27 1 -0.68 1.21 1 -0.06 0.88 1 0 0.66 1 -0.61 0.76 1 -0.08 -0.77 0 -0.34 -0.41 0 -0.77 -0.04 0 0.6 0.74 1 0.84 0.68 0 0.55 1.02 1 0.4 0.09 0 0.08 -0.31 0 -0.98 1.07 1 -0.69 -0.48 0 -0.88 -0.14 0 0.47 -0.25 0 -0.07 0.14 1 0.64 1.13 1 -0.81 1.53 1 0.58 0.99 1 -0.11 -0.4 0 -0.14 -0.49 0 0.28 0.3 1 0.65 0.24 0 -0.27 0.52 1 -0.41 0.57 1 -0.06 0.17 1 -0.57 0.15 0 0.65 -0.52 0 0.68 1.16 1 0.36 0.52 1 0.71 0.95 1 -0.06 -0.17 0 0.74 0.18 0 -0.02 -0.46 0 -0.89 0.11 0 0.7 0.42 0 -0.99 1.96 1 -0.98 1.55 1 0 -0.56 0 0.33 1.02 1 -0.51 1.22 1 0.27 0.69 1 0.96 0.95 1 -0.06 0.16 1 0.33 0.27 1 -0.37 0.91 1 0.65 1.25 1 -0.99 0 0 0.99 1.64 1 -0.93 1.63 1 0.35 -0.79 0 0.25 0.31 1 0.75 0.8 1 -0.2 0.27 1 0.02 -0.13 0 -0.28 0.37 1 -0.09 -0.41 0 0.81 0.81 1 0.69 0.56 1 0 -0.75 0 0.43 0.1 0 0.84 1.44 1 0.81 -0.32 0 -0.47 0.73 1 -0.06 0.02 1 -0.26 0.55 1 0.06 0.76 1 -0.18 -0.87 0 -0.9 0.89 1 -0.74 0.19 0 0.65 0.15 0 0.28 -0.43 0 0.15 -0.37 0 0.51 -0.07 0 0.28 0.25 1 -0.39 0.99 1 -0.6 1 1 0.33 0.13 1 0.52 -0.52 0 0.47 -0.04 0 0.96 1.9 1 0.06 0.46 1 0.51 0.13 0 0.48 0.54 1 -0.95 0.5 0 -0.66 0.94 1 0.37 0 0 0.66 1.02 1 0.34 -0.67 0 -0.28 0 0 -0.44 0.53 1 0.37 0.27 1 -0.88 0.91 1 -0.38 -0.19 0 -0.25 0.59 1 0.23 -0.56 0 0.81 0.58 0 -0.85 0.99 1 -0.6 -0.55 0 0.85 0.1 0 0.2 0.19 1 -0.49 -0.67 0 -0.49 1.04 1 -0.6 1.01 1 0.5 -0.66 0 -0.19 0.02 0 0.22 -0.47 0 -0.52 -0.1 0 -0.97 0.03 0 0.02 -0.59 0 0.98 1.1 1 -0.59 0.87 1 -0.42 -0.31 0 -0.02 0.25 1 0.99 1.25 1 -0.14 0.16 1 -0.5 -0.48 0 -0.17 0.17 1 0.35 -0.64 0 0.67 -0.39 0 -0.5 -0.38 0 -0.13 -0.71 0 -0.9 0.55 0 -0.22 0.05 1 0.72 -0.45 0 0.57 0.7 1 -0.08 -0.6 0 -0.98 1.38 1 0.3 0.01 0 -0.26 -0.82 0 -0.59 0.94 1 -0.96 0.44 0 -0.69 1.24 1 -0.25 0.8 1 -0.69 0 0 0.75 1.53 1 -0.53 -0.52 0 0.47 -0.11 0 -0.92 0.63 0 -0.75 -0.05 0 -0.77 0.19 0 0.64 -0.27 0 0.68 0.2 0 -0.42 -0.15 0 -0.7 0.39 0 -0.6 0.19 0 0.39 0.28 1 -0.24 -0.23 0 0.68 -0.07 0 -0.62 -0.44 0 -0.83 1.37 1 -0.66 -0.52 0 0.13 -0.02 0 -0.15 0.75 1 -0.65 -0.3 0 0.05 0.05 1 -0.8 1.08 1 -0.42 -0.48 0 -0.59 1.13 1 -0.07 -0.1 0 0.05 -0.16 0
输出

P N P N P N P N N P P P N P N N P N N P N P P N V N N N N P P N P P P N P N N N N N V N P N P N N P N P P N P N N N N N N N N N N P N N N P N N P N P P N P N N
用例4：
输入

250 2 100 -0.71 -0.48 0 0.43 0.3 1 -0.59 0.08 0 -0.33 -0.08 0 0.03 0.14 1 -0.54 -0.44 0 -0.7 1.3 1 0.7 0.35 0 -0.14 -0.65 0 0.28 0.25 1 0.21 -0.25 0 0.07 0.28 1 -0.05 0.53 1 -0.26 -0.25 0 0.32 -0.28 0 -0.11 0.61 1 -0.93 1.6 1 -0.68 1.41 1 0.81 -0.04 0 -0.81 1.02 1 -0.17 -0.29 0 0.76 1.16 1 0.6 -0.29 0 0.7 0.6 1 -0.2 0.64 1 -0.68 -0.33 0 0.49 0.19 0 -0.92 0.83 0 0.38 -0.18 0 0.63 1.22 1 -0.05 -0.07 0 -0.82 0.86 1 -0.87 0.48 0 -0.32 -0.01 0 0.5 -0.06 0 0.63 0.87 1 -0.06 -0.54 0 0.92 1.07 1 0.55 -0.27 0 0.84 0.86 1 -0.66 0.58 1 -0.76 0.52 0 -0.82 0.38 0 0.32 0.99 1 0.04 0.42 1 0 -0.03 0 -0.7 1.03 1 -0.39 0.32 1 0.97 0.47 0 -0.63 0.77 1 0.7 -0.45 0 -0.36 0.15 1 0.3 -0.12 0 -0.12 0.59 1 -0.19 0.52 1 0.35 0.2 1 -0.31 -0.86 0 -0.21 0.9 1 -0.9 1.21 1 -0.74 -0.39 0 -0.7 0.29 0 0.47 0.54 1 -0.99 1.44 1 0.07 -0.62 0 0.7 0.63 1 0.38 0.67 1 -0.06 -0.64 0 -0.45 0.98 1 -0.46 1.12 1 -0.05 0.23 1 0.14 -0.84 0 0.83 0.43 0 -0.81 0.65 0 0.19 0.87 1 -0.09 0.99 1 -0.13 0.74 1 -0.78 0.63 1 -0.49 -0.11 0 -0.39 0.74 1 0.82 0.75 1 -0.53 -0.57 0 0.15 -0.78 0 -0.41 0.33 1 -0.06 0.92 1 0.62 -0.21 0 -0.96 1.41 1 -0.7 1.46 1 -0.66 -0.46 0 -0.87 0.76 1 -0.32 0.89 1 -0.05 0.13 1 -0.03 0.8 1 0.39 0.8 1 -0.29 -0.58 0 -0.43 1.15 1 0.9 1.57 1 -0.12 0.02 1 -0.36 0.37 1 -0.14 0.85 1 0.26 -0.52 0 -0.59 -0.59 0 -0.73 0.87 1 -0.44 -0.7 0 0.52 0.14 0 0.6 0.94 1 0.51 0.52 1 0.1 0.99 1 -0.58 -0.08 0 0.82 1.04 1 -0.07 -0.99 0 0.75 0.47 0 0.13 0.62 1 -0.22 0.17 1 -0.96 0.35 0 -0.72 0.9 1 0.99 1.39 1 -0.03 0.02 1 -0.92 0.66 0 0.76 -0.21 0 -0.95 1.3 1 -0.2 -0.27 0 0.75 0.68 1 -0.87 0.32 0 -0.56 -0.55 0 0.37 -0.83 0 -0.35 0.25 1 -0.44 1.14 1 -0.77 0.44 0 0.94 0.77 0 0.44 0.61 1 -0.2 -0.74 0 -0.02 -0.49 0 -0.41 0.08 0 0.09 -0.39 0 0.98 -0.01 0 0.62 0.44 1 0.39 1.12 1 0.7 1.22 1 0.41 1.13 1 0.06 0.17 1 -0.67 0.94 1 -0.9 0.94 1 0.91 1.09 1 -0.3 -0.7 0 -0.29 0.38 1 -0.79 0.53 0 0.48 -0.03 0 0.57 0.41 1 -0.5 0.2 0 0.43 0.49 1 0.02 0.32 1 -0.67 1.03 1 0.74 0.17 0 0.31 0.34 1 0.31 1.03 1 0.41 0.86 1 -0.91 0.86 1 0.4 -0.54 0 -0.55 0.19 0 0.06 -0.5 0 -0.93 1.3 1 -0.36 0.81 1 -0.18 0.73 1 0.69 0.06 0 -0.69 0.42 0 0.46 0.83 1 0.18 -0.01 0 0.65 -0.2 0 -0.15 0.53 1 -0.22 -0.23 0 -0.18 0.36 1 0.34 -0.27 0 0.37 1.01 1 0.77 0.25 0 0.29 -0.38 0 0.42 1.04 1 -0.77 0.84 1 0.25 0.8 1 -0.75 0.92 1 -0.9 0.8 0 0.5 0.69 1 -0.85 0.01 0 -0.77 1.31 1 0.26 -0.83 0 0.96 1.22 1 0.81 1.49 1 -0.6 0.51 1 0.72 -0.24 0 0.56 0.21 0 0.49 0.24 0 0.45 0.22 1 0.67 0.72 1 -0.15 -0.57 0 -0.79 1.52 1 -0.9 0.7 0 0.26 0.79 1 -0.21 0.07 1 0.13 -0.76 0 0.38 0.17 1 -0.59 0.97 1 -0.07 -0.61 0 -0.42 0.22 1 0.89 1.72 1 0.03 0.37 1 -0.86 1.28 1 0.28 0.48 1 0.31 0.49 1 0.44 -0.58 0 0.08 0.91 1 0.04 0.42 1 -0.44 0.62 1 0.34 -0.04 0 -0.94 1.43 1 0.56 -0.47 0 -0.45 -0.73 0 0.17 0.42 1 -0.92 0.95 1 -0.42 -0.78 0 -0.04 -0.29 0 -0.29 0.04 0 -0.27 0.73 1 -0.55 -0.55 0 -0.97 1.88 1 -0.1 -0.57 0 0.08 -0.1 0 0 -0.99 0 -0.65 -0.28 0 -0.68 1.07 1 -0.87 0.52 0 -0.59 -0.05 0 0.81 1.46 1 0.85 -0.03 0 0.87 1.34 1 -0.63 1.31 1 -0.44 0.83 1 0.35 0 0 -0.54 -0.59 0 0.2 -0.54 0 0.28 0.27 1 -0.45 1.04 1 -0.03 -0.04 0 0.74 0.6 1 0.05 -0.69 0 0.96 1.61 1 -0.11 -0.56 0 0.02 0.14 1 -0.24 0.66 1 -0.38 0.82 1 0.75 0.21 0 0.79 0.32 0 -0.01 -0.59 0 0.95 0.27 0 0.65 0.46 1 0.22 0.7 1 -0.36 -0.81 0 0.34 -0.61 0 -0.45 0.87 1 0.77 0.84 1 0.12 -0.96 0 -0.5 -0.45 0 0.07 0.23 1 -0.87 1.17 1 -0.06 0.31 1 0.63 0.46 1 0.26 -0.28 0 -0.67 0.75 1 -0.6 0.05 0 -0.38 -0.55 0 0.2 0.27 1 0.09 -0.5 0 0.77 0.47 0 0.66 0.59 1 0.55 0.95 1 -0.36 1.12 1 0.99 1.12 1 -0.25 0.59 1 0.35 0.04 0 -0.79 1.29 1 0.86 0.67 0 0.2 0.6 1 0.84 0.24 0 -0.21 0.41 1 0.78 0.22 0 0.56 0.49 1 -0.61 -0.34 0 0.46 0.84 1 0.69 -0.08 0 0.68 0.43 0 0.74 0.8 1 0.56 0.29 0 -0.11 0.15 1 -0.85 -0.14 0 0.59 0.4 1 -0.39 0.1 0 -0.79 0.76 1 0.13 -0.47 0 0.03 0.46 1 -0.5 0.56 1 0.15 -0.92 0 0.03 -0.86 0 0.06 0.98 1 0.28 -0.7 0 0.49 0.93 1 -0.19 -0.43 0 -0.74 0.31 0 0.38 -0.39 0 -0.84 1.68 1 0.31 0.98 1 0.3 0.37 1 0.61 0.43 1 0.99 0.32 0 0.81 -0.01 0 0.38 0.36 1 0.27 0.5 1 0.63 1.15 1 0.86 0.44 0 -0.19 0.97 1 -0.46 0.38 1 0.33 0.04 0 0.88 0.47 0 -0.12 0.62 1 0.75 -0.04 0 -0.83 -0.17 0 0.47 -0.22 0 -0.39 -0.77 0 -0.4 0.39 1 0.3 0.23 1 -0.12 -0.61 0 0 -0.19 0 0.31 -0.58 0 -0.01 -0.38 0 -0.59 0.05 0 -0.57 -0.55 0 0.21 0.01 0 -0.16 -0.13 0 -0.25 0.77 1 0.7 1.09 1 0.17 -0.41 0 -0.51 -0.52 0 0.35 -0.29 0 -0.62 1.03 1 -0.06 -0.47 0 -0.28 -0.51 0 -0.48 0.87 1 -0.84 0.1 0 0.93 1.57 1 -0.75 -0.1 0 -0.24 -0.62 0 0.74 1.52 1 -0.58 -0.42 0
输出

N N P P N N P P N N P P P P N P N N P N N P P P P P N P P P N P N P N P N V P V P N P N P N P P N N P N P N N N P P P P N N P P P N P P N N P N N N N P P N N N N N N N N P P N N N P N N P N P N N P N
